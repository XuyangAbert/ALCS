{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3uORWZ9K+A5cNGkg75hOp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuyangAbert/ALCS/blob/main/example_oufsfc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/XuyangAbert/OUFSDFC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lneolwqn_dGi",
        "outputId": "03103e07-e600-4f2c-d96f-6044c83432c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OUFSDFC'...\n",
            "remote: Enumerating objects: 429, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 429 (delta 9), reused 4 (delta 4), pack-reused 411\u001b[K\n",
            "Receiving objects: 100% (429/429), 25.69 MiB | 11.09 MiB/s, done.\n",
            "Resolving deltas: 100% (234/234), done.\n",
            "Updating files: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4d_zMYf4-uX0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from math import exp, ceil\n",
        "import numpy.matlib as b\n",
        "from sklearn.preprocessing import normalize\n",
        "import time\n",
        "from OUFSDFC.Codes.entropy_estimators import *\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import f1_score,accuracy_score, precision_score, recall_score\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Change the filename here:\n",
        "global filename, Batchsize\n",
        "filename = '/content/OUFSDFC/Codes/ALLMALL/ALLAML.csv'\n",
        "# Change the chunk size here:\n",
        "Batchsize = 250\n",
        "\n",
        "def Input():\n",
        "    # Read the data from the txt file\n",
        "    sample = pd.read_csv(filename,header=None)\n",
        "    (N, L) = np.shape(sample)\n",
        "    dim = L - 1\n",
        "    label1 = sample.iloc[:,L-1]\n",
        "    label = label1.values\n",
        "    data = sample.iloc[:,0:dim]\n",
        "    NewData = Pre_Data(data)\n",
        "\n",
        "    return NewData,label\n",
        "def Pre_Data(data):\n",
        "    [N,L] = np.shape(data)\n",
        "    NewData = np.zeros((N,L))\n",
        "    for i in range(L):\n",
        "        Temp = data.iloc[:,i]\n",
        "        if max(Temp)==0:\n",
        "            NewData[:,i] = 0\n",
        "        else:\n",
        "            NewData[:,i] = Temp\n",
        "    return NewData\n",
        "\n",
        "def Distribution_Est(histfeatures, data, dim):\n",
        "    L =  dim\n",
        "    DC_mean = np.zeros(L)\n",
        "    DC_std = np.zeros(L)\n",
        "    for i in range(dim):\n",
        "        TempClass = data[:, i]\n",
        "        DC_mean[i] = np.mean(TempClass)\n",
        "        DC_std[i] = np.std(TempClass)\n",
        "    return DC_mean, DC_std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Feature_Dist(histfeatures, data, dim):\n",
        "    Dist = []\n",
        "    L = dim\n",
        "    Var = np.var(data, axis=0)\n",
        "    Corr = np.corrcoef(data.T)\n",
        "    DisC = np.zeros((L,L))\n",
        "    # histf = histfeatures[:,0]\n",
        "    for i in range(L):\n",
        "        for j in range(i, L):\n",
        "            DisC[i,j] = KLD_Cal(data,i,j,Var,Corr)\n",
        "            # DisC[i, j] = Sym_Cal(data, i, j)\n",
        "            DisC[j, i] = DisC[i, j]\n",
        "            Dist.append(DisC[i, j])\n",
        "    return DisC,Dist\n",
        "\n",
        "def KLD_Cal(data,i,j,Var,Corr):\n",
        "    Var1 = Var[i]\n",
        "    Var2 = Var[j]\n",
        "    P = Corr[i,j]\n",
        "    Sim = Var1 + Var2 - ((Var1 + Var2)**2 - 4 * Var1 * Var2 * (1 - P**2))**0.5\n",
        "    D_KL = Sim / (Var1 + Var2)\n",
        "    return D_KL\n",
        "\n",
        "def Sym_Cal(data,i,j):\n",
        "    if len(np.unique(data[:,i]))==0 or len(np.unique(data[:,i]))==0:\n",
        "        D_KL = 1\n",
        "    else:\n",
        "        I_ij = midd(data[:,i],data[:,j])\n",
        "        H_I = entropyd(data[:,i])\n",
        "        H_J = entropyd(data[:,j])\n",
        "        if H_I == 0 or H_J == 0:\n",
        "            D_KL = 0\n",
        "        else:\n",
        "            D_KL = 1 - 2*(I_ij)/(H_I + H_J)\n",
        "    return D_KL\n",
        "\n",
        "def fitness_cal(histsummary, DisC, DC_means, DC_std, data, StdF, gamma, histStdF):\n",
        "    fitness = np.zeros(len(DC_means))\n",
        "    # print(np.shape(fitness))\n",
        "    for i in range(len(DC_means)):\n",
        "        TempSum = 0\n",
        "        for j in range(len(DC_means)):\n",
        "            if j != i:\n",
        "                D = DisC[i,j]\n",
        "                TempSum = TempSum + (exp(- (D**2) / StdF))**gamma\n",
        "                # TempSum = TempSum + (exp(- (D) / StdF))**gamma\n",
        "        fitness[i] = TempSum\n",
        "    if len(histStdF) == 0:\n",
        "        updatedfit = fitness\n",
        "    else:\n",
        "        updatedfit = fitness_update(DisC, histsummary, histStdF, fitness, StdF, gamma)\n",
        "    return updatedfit\n",
        "    # return fitness\n",
        "\n",
        "def fitness_update(DisC, histsummary, histStdF, rawfitness, StdF, gamma):\n",
        "    num_hist = np.shape(histsummary)[0]\n",
        "    num_fit = len(rawfitness)\n",
        "    histdist = DisC[:, :num_hist]\n",
        "    histfitness = histsummary[:,1]\n",
        "    fitness_new = rawfitness\n",
        "    for i in range(num_fit):\n",
        "        curr_dist = histdist[i, :]\n",
        "        n_idx = np.argmin(curr_dist)\n",
        "        n_dist = np.min(curr_dist)\n",
        "        if i < num_hist:\n",
        "            fitness_new[i] = histsummary[n_idx,1]\n",
        "        else:\n",
        "            fitness_new[i] += (exp(- (n_dist**2) / StdF))**gamma * (histfitness[n_idx] ** (histStdF[-1]/StdF))\n",
        "            # fitness_new[i] += (exp(- (n_dist) / StdF))**gamma * (histfitness[n_idx] ** (histStdF[-1]/StdF))\n",
        "    return fitness_new\n",
        "\n",
        "def Pseduo_Peaks(DisC, Dist, DC_Mean, DC_Std, data, fitness, StdF, gamma):\n",
        "\n",
        "    # The temporal sample space in terms of mean and standard deviation\n",
        "    sample = np.vstack((DC_Mean,DC_Std)).T\n",
        "    # Spread= np.max(Dist)\n",
        "    # Search Stage of Pseduo Clusters at the temporal sample space\n",
        "    # NeiRad = 0.1 * StdF\n",
        "    NeiRad = 0.01*np.mean(Dist) # 0.2\n",
        "    # NeiRad = (StdF/gamma)\n",
        "    i = 0\n",
        "    marked = []\n",
        "    C_Indices = np.arange(1, len(DC_Mean)+1) # The pseduo Cluster label of features\n",
        "    PeakIndices = []\n",
        "    Pfitness = []\n",
        "    co = []\n",
        "    F = fitness\n",
        "    while True:\n",
        "        PeakIndices.append(np.argmax(F))\n",
        "        Pfitness.append(np.max(F))\n",
        "\n",
        "        indices = NeighborSearch(DisC, data, sample, PeakIndices[i], marked, NeiRad)\n",
        "\n",
        "        C_Indices[indices] = PeakIndices[i]\n",
        "        if len(indices) == 0:\n",
        "            indices=[PeakIndices[i]]\n",
        "\n",
        "        co.append(len(indices)) # Number of samples belong to the current\n",
        "    # identified pseduo cluster\n",
        "        marked = np.concatenate(([marked,indices]))\n",
        "\n",
        "        # Fitness Proportionate Sharing\n",
        "        F = Sharing(F, indices)\n",
        "\n",
        "        # Check whether all of samples has been assigned a pseduo cluster label\n",
        "        if np.sum(co) >= (len(F)):\n",
        "            PeakIndices = np.unique(PeakIndices)\n",
        "            C_Indices = Close_FCluster(PeakIndices, DisC, np.shape(DisC)[0])\n",
        "            break\n",
        "\n",
        "        i=i+1 # Expand the size of the pseduo cluster set by 1\n",
        "    return PeakIndices,Pfitness,C_Indices\n",
        "\n",
        "def NeighborSearch(DisC, data, sample, P_indice, marked, radius):\n",
        "    Cluster = []\n",
        "    for i in range(np.shape(sample)[0]):\n",
        "        if i not in marked:\n",
        "            Dist = DisC[i, P_indice]\n",
        "            if Dist <= radius:\n",
        "                Cluster.append(i)\n",
        "    Indices = Cluster\n",
        "    return Indices\n",
        "#---------------------------------------------------------------------------------------------------\n",
        "def Sharing(fitness, indices):\n",
        "    newfitness = fitness\n",
        "    sum1 = 0\n",
        "    for j in range(len(indices)):\n",
        "        sum1 = sum1 + fitness[indices[j]]\n",
        "    for th in range(len(indices)):\n",
        "            newfitness[indices[th]] = fitness[indices[th]] / (1+sum1)\n",
        "\n",
        "    return newfitness\n",
        "#-----------------------------------------------------------------------------------------------------\n",
        "def Pseduo_Evolve(DisC, PeakIndices, PseDuoF, C_Indices, DC_Mean, DC_Std, data, fitness, StdF, gamma):\n",
        "\n",
        "    # Initialize the indices of Historical Pseduo Clusters and their fitness values\n",
        "    HistCluster = PeakIndices\n",
        "    HistClusterF = PseDuoF\n",
        "    while True:\n",
        "        # Call the merge function in each iteration\n",
        "        [Cluster,Cfitness,F_Indices] = Pseduo_Merge(DisC, HistCluster, HistClusterF, C_Indices, DC_Mean, DC_Std, data, fitness, StdF, gamma)\n",
        "        # Check for the stablization of clutser evolution and exit the loop\n",
        "        if len(np.unique(Cluster)) == len(np.unique(HistCluster)) or len(Cluster) == 1:\n",
        "            break\n",
        "\n",
        "        # Update the feature indices of historical pseduo feature clusters and\n",
        "        # their corresponding fitness values\n",
        "\n",
        "        HistCluster=Cluster\n",
        "        HistClusterF=Cfitness\n",
        "        C_Indices = F_Indices\n",
        "    # Compute final evolved feature cluster information\n",
        "    FCluster = np.unique(Cluster)\n",
        "    Ffitness = Cfitness\n",
        "    C_Indices = Close_FCluster(FCluster, DisC, np.shape(DisC)[0])\n",
        "\n",
        "    return FCluster, Ffitness, C_Indices\n",
        "#----------------------------------------------------------------------------------------------------------\n",
        "def Pseduo_Merge(DisC, PeakIndices, PseDuoF, C_Indices, DC_Mean, DC_Std, data, fitness, StdF, gamma):\n",
        "    # Initialize the pseduo feature clusters lables for all features\n",
        "    F_Indices = C_Indices\n",
        "    # Initialize the temporal sample space for feature means and stds\n",
        "    sample = np.vstack((DC_Mean,DC_Std)).T\n",
        "    ML = [] # Initialize the merge list as empty\n",
        "    marked = [] #List of checked Pseduo Clusters Indices\n",
        "    Unmarked = [] # List of unmerged Pseduo Clusters Indices\n",
        "    for i in range(len(PeakIndices)):\n",
        "            M = 1 # Set the merge flag as default zero\n",
        "            MinDist = math.inf # Set the default Minimum distance between two feature clusters as infinite\n",
        "            MinIndice = 0 # Set the default Neighboring feature cluster indices as zero\n",
        "            # Check the current Pseduo Feature Cluster has been evaluated or not\n",
        "            if PeakIndices[i] not in marked:\n",
        "                for j in range(len(PeakIndices)):\n",
        "                        if j != i:\n",
        "                            # Divergence Calculation between two pseduo feature clusters\n",
        "                            D = DisC[PeakIndices[i], PeakIndices[j]]\n",
        "                            if MinDist > D:\n",
        "                                MinDist = D\n",
        "                                MinIndice = j\n",
        "                if MinIndice != 0:\n",
        "                    # Current feature pseduo cluster under check\n",
        "                    Current = sample[PeakIndices[i],:]\n",
        "                    CurrentFit = PseDuoF[i]\n",
        "                    # Neighboring feature pseduo cluster of the current checked cluster\n",
        "                    Neighbor = sample[PeakIndices[MinIndice],:]\n",
        "                    NeighborFit = PseDuoF[MinIndice]\n",
        "\n",
        "                    # A function to identify the bounady feature instance between two\n",
        "                    # neighboring pseduo feature clusters\n",
        "                    BP=Boundary_Points(DisC, F_Indices,data, PeakIndices[i], PeakIndices[MinIndice])\n",
        "                    BPF=fitness[BP]\n",
        "                    if BPF < 0.85*min(CurrentFit,NeighborFit): # 0.95\n",
        "                        M = 0 # Change the Merge flag\n",
        "                    else:\n",
        "                        M = 1\n",
        "\n",
        "                    if M == 1:\n",
        "                        ML.append([PeakIndices[i],PeakIndices[MinIndice]])\n",
        "                        marked.append(PeakIndices[i])\n",
        "                        marked.append(PeakIndices[MinIndice])\n",
        "                    else:\n",
        "                        Unmarked.append(PeakIndices[i])\n",
        "    NewPI = []\n",
        "    # Update the pseduo feature clusters list with the obtained mergelist\n",
        "    for m in range(np.shape(ML)[0]):\n",
        "        # print(ML[m][0],ML[m][1])\n",
        "        if fitness[ML[m][0]] > fitness[ML[m][1]]:\n",
        "            NewPI.append(ML[m][0])\n",
        "            F_Indices[C_Indices==ML[m][1]] = ML[m][0]\n",
        "        else:\n",
        "            NewPI.append(ML[m][1])\n",
        "            F_Indices[C_Indices==ML[m][0]] = ML[m][1]\n",
        "    # Update the pseduo feature clusters list with pseduo clusters that have not appeared in the merge list\n",
        "    for n in range(len(PeakIndices)):\n",
        "        if PeakIndices[n] in Unmarked:\n",
        "            NewPI.append(PeakIndices[n])\n",
        "\n",
        "    # Updated pseduo feature clusters information after merging\n",
        "    FCluster = np.unique(NewPI)\n",
        "    if len(FCluster) == 0:\n",
        "        return [], [], []\n",
        "    Ffitness = fitness[FCluster]\n",
        "    F_Indices = Close_FCluster(FCluster, DisC, np.shape(DisC)[0])\n",
        "    return FCluster, Ffitness, F_Indices"
      ],
      "metadata": {
        "id": "4xkBsgys-3Xj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------Boundary Feature Identification------------------------------------#\n",
        "def Boundary_Points(DisC, F_Indices, data, Current, Neighbor):\n",
        "\n",
        "    [N, dim] = np.shape(data)\n",
        "    TempCluster1 = np.where(F_Indices == Current)\n",
        "    TempCluster2 = np.where(F_Indices == Neighbor)\n",
        "\n",
        "    TempCluster = np.append(TempCluster1,TempCluster2)\n",
        "\n",
        "    D = []\n",
        "\n",
        "    for i in range(len(TempCluster)):\n",
        "        D1 = DisC[TempCluster[i], Current]\n",
        "        D2 = DisC[TempCluster[i], Neighbor]\n",
        "\n",
        "        D.append(abs(D1 - D2))\n",
        "\n",
        "    if not D:\n",
        "        BD = Current\n",
        "    else:\n",
        "        FI = np.argmin(D)\n",
        "        BD = TempCluster[FI]\n",
        "#    BD = TempCluster[FI]\n",
        "\n",
        "    return BD\n",
        "\n",
        "def FC_evolve(DisC, histsummary, FCluster, Ffitness, C_Indices, Features, fitness, t, Batchsize):\n",
        "    histFclusters = histsummary[:, 0]\n",
        "    histFfitness = histsummary[:, 1]\n",
        "    currentFcluster = FCluster\n",
        "    currentFfitness = Ffitness\n",
        "    newFcluster = hist_merge(DisC, currentFcluster, currentFfitness, C_Indices,\n",
        "                                                           histFclusters, histFfitness, t, Batchsize)\n",
        "    return newFcluster\n",
        "\n",
        "def hist_merge(DisC, fitness, currentFcluster, currentFfitness, C_Indices, histFcluster, histFfitness, t, Batchsize):\n",
        "    num_cur = np.shape(currentFcluster)[0]\n",
        "    num_hist = np.shape(histFcluster)[0]\n",
        "    merge_list = []\n",
        "    ml1 = []\n",
        "    ml2 = []\n",
        "#    unique_c = np.unique(C_Indices)\n",
        "    for i in range(num_cur):\n",
        "        M = False\n",
        "        tempdist = DisC[i+num_hist, : num_hist]\n",
        "        nearhist = np.argmin(tempdist)\n",
        "#        neighborhist = histFcluster[nearhist]\n",
        "        neighborhistf = histFfitness[nearhist]\n",
        "        current = int(currentFcluster[i])\n",
        "        currentf = currentFfitness[i]\n",
        "        # Identify the boundary features from the current feature clusters\n",
        "        temp_clusteridx = np.where(C_Indices == current)[0]\n",
        "        if len(temp_clusteridx) == 0:\n",
        "            temp_clusteridx = current - t * Batchsize\n",
        "            BP = temp_clusteridx\n",
        "        else:\n",
        "            temp_clusterd1 = DisC[temp_clusteridx + num_hist, current-t*Batchsize]\n",
        "            temp_clusterd2 = DisC[temp_clusteridx + num_hist, nearhist]\n",
        "            bet_clusters = temp_clusterd1 + temp_clusterd2\n",
        "            sort_idx = np.argsort(bet_clusters)\n",
        "            if len(sort_idx) == 1:\n",
        "                BP = temp_clusteridx[sort_idx[0]]\n",
        "            else:\n",
        "                if sort_idx[0] != current:\n",
        "                    BP = temp_clusteridx[sort_idx[0]]\n",
        "                else:\n",
        "                    BP = temp_clusteridx[sort_idx[1]]\n",
        "        BPF = fitness[BP]\n",
        "        if BPF < 0.85 * min(currentf, neighborhistf):\n",
        "            M = False\n",
        "        else:\n",
        "            M = True\n",
        "        if M:\n",
        "            if len(merge_list) == 0:\n",
        "                merge_list = [[i, nearhist]]\n",
        "                ml1 = [i]\n",
        "                ml2 = [nearhist]\n",
        "            else:\n",
        "                merge_list.append([i, nearhist])\n",
        "                ml1.append(i)\n",
        "                ml2.append(nearhist)\n",
        "    evolved = []\n",
        "#    evolvedf = []\n",
        "\n",
        "    for k in range(np.shape(merge_list)[0]):\n",
        "        idx1 = merge_list[k][0]\n",
        "        # print(idx1)\n",
        "        idx2 = merge_list[k][1]\n",
        "        # print(idx2)\n",
        "        if currentFfitness[idx1] > histFfitness[idx2]:\n",
        "            evolved.append(currentFcluster[idx1])\n",
        "            # evolvedf.append(fitness[idx1])\n",
        "        else:\n",
        "            evolved.append(histFcluster[idx2])\n",
        "            # evolvedf.append(fitness[idx2])\n",
        "    for l1 in range(len(histFcluster)):\n",
        "        if l1 not in ml2:\n",
        "            evolved.append(int(histFcluster[l1]))\n",
        "            # evolvedf.append(fitness[l1])\n",
        "    for l2 in range(len(currentFcluster)):\n",
        "        if l2 not in ml1:\n",
        "            evolved.append(currentFcluster[l2])\n",
        "            # evolvedf.append(fitness[num_hist+l2])\n",
        "    evolved = np.unique(evolved)\n",
        "    evolved = evolved.astype(int)\n",
        "    # evolvedf = evolvedf[evolved - t * Batchsize]\n",
        "    # evolvedI = Close_FCluster(evolved, DisC, dim)\n",
        "    # return evolved, evolvedf, evolvedI\n",
        "    return evolved\n",
        "\n",
        "def Close_FCluster(FCluster, DisC, dim):\n",
        "    F_Indices = np.arange(dim)\n",
        "    for i in range(dim):\n",
        "        dist_fcluster = DisC[i, FCluster]\n",
        "        F_Indices[i] = FCluster[np.argmin(dist_fcluster)]\n",
        "    return F_Indices\n",
        "\n",
        "#------------------Pseudo Feature Generation----------------#\n",
        "    \"\"\"Generate the Pseudo Features using the Gaussian Distribution\n",
        "    \"\"\"\n",
        "def PseduoGeneration(PseP,N):\n",
        "    # Extract the Pseudo Feature Means and Standard deviations\n",
        "    Pse_Mean = PseP[:,0]\n",
        "    Pse_Std = PseP[:,1]\n",
        "    # Initialize the Pseudo Data as empty array\n",
        "    Data = np.zeros((N,len(Pse_Mean)))\n",
        "    # Generate the Pseudo features using Gaussian Distribution\n",
        "    for i in range(len(Pse_Mean)):\n",
        "        Data[:, i] = (np.repeat(Pse_Mean[i],N) + Pse_Std[i] * np.random.randn(N)).T\n",
        "    return Data\n",
        "\n",
        "def Psefitness_cal( PseP, sample, data, PseduoData, StdF, gamma):\n",
        "\n",
        "    OriFN = np.shape(sample)[0]\n",
        "    PN = np.shape(PseP)[0]\n",
        "    PsePF = np.zeros(PN)\n",
        "    for i in range(PN):\n",
        "        TempSum = 0\n",
        "        for j in range(OriFN):\n",
        "            Var1 = np.var(data[:,j])\n",
        "            Var2 = np.var(PseduoData[:,i])\n",
        "\n",
        "            P = np.corrcoef(data[:,j],PseduoData[:,i])[0,1]\n",
        "\n",
        "            Sim = Var1 + Var2 - ((Var1 + Var2)**2 - 4 * Var1 * Var2 * (1 - P**2))**0.5\n",
        "\n",
        "            D_KL = Sim / (Var1 + Var2)\n",
        "\n",
        "            TempSum = TempSum + (math.exp(-(D_KL**2)/StdF))**gamma\n",
        "        PsePF[i] = TempSum\n",
        "    return PsePF"
      ],
      "metadata": {
        "id": "PEpeFFUT_McV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------Define Functions for Performance Evaluation---------------#\n",
        "\"\"\"\n",
        "Three Different Classifiers are defined here: Decision Tree, K-Nearest Neighborhood\n",
        "and Naive Bayes Classifier\n",
        "\"\"\"\n",
        "def Performance_Eval_DT(X1,X2,Y):\n",
        "\n",
        "    clf1 = tree.DecisionTreeClassifier()\n",
        "    clf2 = tree.DecisionTreeClassifier()\n",
        "\n",
        "    scores1 = cross_val_score(clf1, X1, Y, cv=10)\n",
        "    scores2 = cross_val_score(clf2, X2, Y, cv=10)\n",
        "\n",
        "    return scores1,scores2\n",
        "\n",
        "def Performance_Eval_NB(X1,X2,Y):\n",
        "\n",
        "    clf1 = GaussianNB()\n",
        "    clf2 = GaussianNB()\n",
        "\n",
        "    scores1 = cross_val_score(clf1, X1, Y, cv=10)\n",
        "    scores2 = cross_val_score(clf2, X2, Y, cv=10)\n",
        "\n",
        "    return scores1,scores2\n",
        "\n",
        "def Performance_Eval_KNN(X1,X2,Y):\n",
        "\n",
        "    clf1 = KNeighborsClassifier(n_neighbors=5)\n",
        "    clf2 = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "    scores1 = cross_val_score(clf1, X1, Y, cv=10)\n",
        "    scores2 = cross_val_score(clf2, X2, Y, cv=10)\n",
        "\n",
        "    return scores1,scores2"
      ],
      "metadata": {
        "id": "SUWE0c5__Ot2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------------------------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    [data, label] = Input()\n",
        "    [N, dim] = np.shape(data)\n",
        "    label = label.reshape(N,)\n",
        "    group_kfold = StratifiedKFold(n_splits=10)\n",
        "    iterations = 10\n",
        "    acck_hist, f1k_hist, sfk_hist = [], [], []\n",
        "    accd_hist, f1d_hist, sfd_hist = [], [], []\n",
        "    for it in range(iterations):\n",
        "        fold_id = 1\n",
        "        acck_cross, f1k_cross, sfk_cross = [], [], []\n",
        "        accd_cross, f1d_cross, sfd_cross = [], [], []\n",
        "        for train_idx, test_idx in group_kfold.split(data,label):\n",
        "            train_data = data[train_idx,:]\n",
        "            train_label = label[train_idx]\n",
        "            test_data = data[test_idx,:]\n",
        "            test_label = label[test_idx]\n",
        "            NumofBatches = round(dim/Batchsize)\n",
        "            Extract_FIndices = []\n",
        "            FCluster = []\n",
        "            histsummary = []\n",
        "            histStdF = []\n",
        "            for i in range(NumofBatches):\n",
        "                if i < NumofBatches-1:\n",
        "                    process_fidx = np.arange(i * Batchsize, (i + 1) * Batchsize)\n",
        "                    Features = train_data[:, process_fidx]\n",
        "                else:\n",
        "                    process_fidx = np.arange(i*Batchsize, dim)\n",
        "                    Features = train_data[:, process_fidx]\n",
        "                num_hist = np.shape(histsummary)[0]\n",
        "                if num_hist == 0:\n",
        "                    concate_features = Features\n",
        "                else:\n",
        "                    histfidx  = np.asarray(histsummary[:, 0])\n",
        "                    histfidx = histfidx.astype(int)\n",
        "                    concate_features = np.concatenate([train_data[:, histfidx], Features], axis = 1)\n",
        "                N_F = np.shape(concate_features)[1]\n",
        "                [DC_means, DC_std] = Distribution_Est(histsummary, concate_features, N_F)\n",
        "                [DisC, Dist] = Feature_Dist(histsummary, concate_features, N_F)\n",
        "                # print('Batch ' + str(i) + ' Finished!')\n",
        "                StdF = max(Dist)\n",
        "                gamma = 5\n",
        "\n",
        "                fitness = fitness_cal(histsummary, DisC, DC_means, DC_std, Features, StdF, gamma, histStdF)\n",
        "                oldfitness = np.copy(fitness)\n",
        "\n",
        "                # Extract the information needed for processing the current feature chunk\n",
        "                if i == 0:\n",
        "                    disc_curr = DisC\n",
        "                    dist_curr = Dist\n",
        "                    fitness_curr = fitness\n",
        "                    oldfit = np.copy(fitness)\n",
        "                    curr_mean = DC_means\n",
        "                    curr_std = DC_std\n",
        "                else:\n",
        "                    disc_curr = DisC[num_hist:, num_hist:]\n",
        "                    dist_curr = Dist[num_hist:]\n",
        "                    fitness_curr = fitness[num_hist:]\n",
        "                    oldfit = np.copy(fitness[num_hist:])\n",
        "                    curr_mean = DC_means[num_hist:]\n",
        "                    curr_std = DC_std[num_hist:]\n",
        "                [PeakIndices, Pfitness, C_Indices] = Pseduo_Peaks(disc_curr, dist_curr, curr_mean, curr_std, Features, fitness_curr,\n",
        "                                                                  StdF, gamma)\n",
        "                fitness = oldfitness\n",
        "                # Pseduo Clusters Infomormation Extraction\n",
        "                PseDuo = PeakIndices  # Pseduo Feature Cluster centers\n",
        "                PseDuoF = Pfitness  # Pseduo Feature Clusters fitness values\n",
        "                if len(PseDuo) > 1:\n",
        "                    [FCluster, Ffitness, C_Indices] = Pseduo_Evolve(disc_curr, PeakIndices, PseDuoF, C_Indices, curr_mean,\n",
        "                                                                    curr_std, Features, oldfit, StdF, gamma)\n",
        "                else:\n",
        "                    FCluster = PeakIndices\n",
        "                    Ffitness = PseDuoF\n",
        "                if i > 0:\n",
        "                    histFcluster = np.asarray(histsummary[:, 0])\n",
        "                    histFcluster = histFcluster.astype(int)\n",
        "                    histFfitness = histsummary[:, 1]\n",
        "                    FCluster2 = FCluster + i * Batchsize\n",
        "                    C_Indices2 = C_Indices + (i * Batchsize)\n",
        "                    EvolvedFC = hist_merge(DisC, fitness_curr, FCluster2, Ffitness, C_Indices2, histFcluster, histFfitness, i, Batchsize)\n",
        "                    EvolvedF = []\n",
        "                    for k in range(len(EvolvedFC)):\n",
        "                        if EvolvedFC[k] in histFcluster:\n",
        "                            histidx = np.where(histFcluster == EvolvedFC[k])[0]\n",
        "                            if len(EvolvedF) == 0:\n",
        "                                EvolvedF = histFfitness[histidx]\n",
        "                            else:\n",
        "                                EvolvedF = np.append(EvolvedF, histFfitness[histidx])\n",
        "                        else:\n",
        "                            curridx = np.where(FCluster == (EvolvedFC[k] - i*Batchsize))[0]\n",
        "                            if len(EvolvedF) == 0:\n",
        "                                EvolvedF = fitness[curridx]\n",
        "                            else:\n",
        "                                EvolvedF = np.append(EvolvedF, fitness[curridx])\n",
        "                else:\n",
        "                    EvolvedFC = FCluster\n",
        "                    EvolvedF = Ffitness\n",
        "                SF = EvolvedFC\n",
        "                SFfit = EvolvedF\n",
        "                current_summary = np.asarray([SF, SFfit])\n",
        "                current_summary = current_summary.T\n",
        "                histsummary = current_summary\n",
        "                histStdF.append(StdF)\n",
        "            Extract_FIndices = SF\n",
        "            Extract_FIndices = Extract_FIndices.astype(int)\n",
        "\n",
        "            clf1 = KNeighborsClassifier(n_neighbors=5)\n",
        "            clf2 = tree.DecisionTreeClassifier()\n",
        "            clf1 = clf1.fit(train_data[:, Extract_FIndices], train_label)\n",
        "            clf2 = clf2.fit(train_data[:, Extract_FIndices], train_label)\n",
        "            predict_label1 = clf1.predict(test_data[:, Extract_FIndices])\n",
        "            predict_label2 = clf2.predict(test_data[:, Extract_FIndices])\n",
        "\n",
        "            accuracy1 = accuracy_score(test_label, predict_label1)\n",
        "            f11 = f1_score(test_label, predict_label1,average='macro')\n",
        "\n",
        "            accuracy2 = accuracy_score(test_label, predict_label2)\n",
        "            f12 = f1_score(test_label, predict_label2,average='macro')\n",
        "\n",
        "            acck_cross.append(accuracy1)\n",
        "            f1k_cross.append(f11)\n",
        "            sfk_cross.append(len(SF))\n",
        "            accd_cross.append(accuracy2)\n",
        "            f1d_cross.append(f12)\n",
        "            sfd_cross.append(len(SF))\n",
        "        acck_hist.append(np.mean(acck_cross))\n",
        "        f1k_hist.append(np.mean(f1k_cross))\n",
        "        sfk_hist.append(np.mean(sfk_cross))\n",
        "\n",
        "        accd_hist.append(np.mean(accd_cross))\n",
        "        f1d_hist.append(np.mean(f1d_cross))\n",
        "        sfd_hist.append(np.mean(sfd_cross))\n",
        "        print(\"Iteration \"+str(it)+\" Finished!\")\n",
        "    print(\"Results on KNN classifiers:\")\n",
        "    print('10 runs average of 10-fold cross validated accuracy: ', np.mean(acck_hist))\n",
        "    print('10 runs average of 10-fold cross validated f1: ', np.mean(f1k_hist))\n",
        "    print('Average number of selected features: ', np.mean(sfk_hist))\n",
        "\n",
        "    print(\"Results on DT classifiers:\")\n",
        "    print('10 runs average of 10-fold cross validated accuracy: ', np.mean(accd_hist))\n",
        "    print('10 runs average of 10-fold cross validated f1: ', np.mean(f1d_hist))\n",
        "    print('Average number of selected features: ', np.mean(sfd_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsTyqHiK_W9L",
        "outputId": "72827472-a3f8-4b9b-9226-1fbec90ede0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 Finished!\n",
            "Iteration 1 Finished!\n",
            "Iteration 2 Finished!\n",
            "Iteration 3 Finished!\n",
            "Iteration 4 Finished!\n",
            "Iteration 5 Finished!\n",
            "Iteration 6 Finished!\n",
            "Iteration 7 Finished!\n",
            "Iteration 8 Finished!\n",
            "Iteration 9 Finished!\n",
            "Results on KNN classifiers:\n",
            "10 runs average of 10-fold cross validated accuracy:  0.9446428571428575\n",
            "10 runs average of 10-fold cross validated f1:  0.9293217893217894\n",
            "Average number of selected features:  53.6\n",
            "Results on DT classifiers:\n",
            "10 runs average of 10-fold cross validated accuracy:  0.9012499999999999\n",
            "10 runs average of 10-fold cross validated f1:  0.8852572150072151\n",
            "Average number of selected features:  53.6\n"
          ]
        }
      ]
    }
  ]
}