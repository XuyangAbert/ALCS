{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuFoz5UQGpewk/J6GB34P6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuyangAbert/ALCS/blob/main/example_alcs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mbPNkUUoIrR"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import pdist,squareform\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy.matlib\n",
        "from math import exp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score,precision_score,auc\n",
        "from sklearn.metrics import accuracy_score,recall_score\n",
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "from sklearn.svm import SVC,LinearSVC\n",
        "start = time.time()\n",
        "def Input():\n",
        "    sample = pd.read_csv('Aggregation.csv',header=None)\n",
        "    [N,L] = np.shape(sample)\n",
        "    dim = L # Extract the num of dimensions\n",
        "    # Extract the label of the data from the data frame\n",
        "    label1 = sample.iloc[:,L-1]\n",
        "    label = label1.values\n",
        "    # Extract the samples from the data frame\n",
        "    data = sample.iloc[:,0:dim-1]\n",
        "    # Normalization Procedure\n",
        "    NewData = Pre_Data(data)\n",
        "    ND = NewData\n",
        "    return ND,label\n",
        "\n",
        "def Pre_Data(data):\n",
        "    [N,L] = np.shape(data)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(data)\n",
        "    NewData = scaler.transform(data)\n",
        "    return NewData\n",
        "\n",
        "def ParamSpe(data):\n",
        "    Buffersize = 1000\n",
        "    PreStd = []\n",
        "    P_Summary = []\n",
        "    PFS = []\n",
        "    T = round(np.shape(data)[0]/Buffersize)\n",
        "    return Buffersize,P_Summary,T,PFS,PreStd\n",
        "\n",
        "def Distance_Cal(data):\n",
        "    D = pdist(data)\n",
        "    Dist = squareform(D)\n",
        "    return Dist\n",
        "\n",
        "def Fitness_Cal(sample,pop,stdData,gamma):\n",
        "    Ns = np.shape(sample)[0]\n",
        "    Np = np.shape(pop)[0]\n",
        "    Newsample = np.concatenate([sample,pop])\n",
        "    Dist = Distance_Cal(Newsample)\n",
        "    fitness = []\n",
        "    for i in range(Np):\n",
        "        distArray = np.power(Dist[i+Ns,0:Ns],2)\n",
        "        temp = np.power(np.exp(-distArray/stdData),gamma)\n",
        "        fitness.append(np.sum(temp))\n",
        "    return fitness\n",
        "\n",
        "def fitness_update(P_Summary,Current,fitness,PreStd,gamma,stdData):\n",
        "    [N,dim] = np.shape(Current)\n",
        "    t_I = len(PreStd)\n",
        "    NewFit = fitness\n",
        "    if len(P_Summary)>0:\n",
        "        PreFit = P_Summary[:,dim]\n",
        "        PreP = P_Summary[:,0:dim]\n",
        "        OldStd = PreStd[t_I-1]\n",
        "        for i in range(N):\n",
        "            fitin = 0\n",
        "            for j in range(np.shape(PreP)[0]):\n",
        "                if np.linalg.norm(Current[i][:]-PreP[j][:])<0.01:\n",
        "                    fitin = PreFit[j]\n",
        "                    break\n",
        "                else:\n",
        "                    d = np.linalg.norm(Current[i][:]-PreP[j][:])\n",
        "                    fitin += (exp(-d**2/stdData)**gamma)*(PreFit[j]**(OldStd/stdData))\n",
        "            NewFit[i] = fitness[i] + fitin\n",
        "    return NewFit\n",
        "\n",
        "def PopInitial(sample,PreMu,PreStd,Buffersize):\n",
        "    [N,L] = np.shape(sample)\n",
        "    pop_Size = round(1*N)\n",
        "    # Compute the statistics of the current data chunk\n",
        "    minLimit = np.min(sample,axis = 0)\n",
        "    meanData = np.mean(sample,axis = 0)\n",
        "    maxLimit = np.max(sample,axis =0)\n",
        "    # Update the statistics of the data stream\n",
        "    meanData = UpdateMean(PreMu,meanData,Buffersize)\n",
        "    PreMu.append(meanData)\n",
        "    # Compute the standard deviation of the current data chunk\n",
        "    MD = np.matlib.repmat(meanData,N,1)\n",
        "    tempSum = np.sum(np.sum((MD-sample)**2,axis=1))\n",
        "    stdData = tempSum / N\n",
        "    # Update the standard deviation of the data stream\n",
        "    stdData = StdUpdate(stdData,PreStd,Buffersize)\n",
        "    # Randonmly Initialize the population indices from the data chunk\n",
        "    pop_Index = np.arange(0,N)\n",
        "    pop = sample[pop_Index,:]\n",
        "    # Calculate the initial niche radius\n",
        "    radius = numpy.linalg.norm((maxLimit-minLimit)) * 0.1\n",
        "    return [stdData,pop_Index,pop,radius,PreMu,PreStd]\n",
        "\n",
        "def UpdateMean(PreMu,meanData,BufferSize):\n",
        "    # Num of the processed data chunk\n",
        "    t_P = len(PreMu)\n",
        "    # Update the mean of the data stream as new data chunk arrives\n",
        "    if t_P==0:\n",
        "        newMu = meanData\n",
        "    else:\n",
        "        oldMu = PreMu[t_P-1][:]\n",
        "        newMu = (meanData + oldMu * t_P) / (t_P + 1)\n",
        "    return newMu\n",
        "\n",
        "def StdUpdate(Std,PreStd,BufferSize):\n",
        "    # Num of the processed data chunk\n",
        "    t_P = len(PreStd)\n",
        "    # Update the variance of the data stream as new data chunk arrives\n",
        "    if t_P==0:\n",
        "        newStd = Std\n",
        "    else:\n",
        "        oldStd = PreStd[t_P-1]\n",
        "        newStd = (Std + oldStd * t_P) / (t_P + 1)\n",
        "    return newStd\n",
        "\n",
        "#------------------------Parameter Estimation----------------------------#\n",
        "def CCA(sample,stdData,Dist):\n",
        "    m = 1\n",
        "    gamma = 5\n",
        "    ep = 0.998 # 0.998\n",
        "    N = np.shape(sample)[0]\n",
        "    while 1:\n",
        "        den1 = []\n",
        "        den2 = []\n",
        "        for i in range(N-1):\n",
        "            Diff = np.power(Dist[i,:],2)\n",
        "            temp1 = np.power(np.exp(-Diff/stdData),gamma*m)\n",
        "            temp2 = np.power(np.exp(-Diff/stdData),gamma*(m+1))\n",
        "            den1.append(np.sum(temp1))\n",
        "            den2.append(np.sum(temp2))\n",
        "        y = np.corrcoef(den1,den2)[0,1]\n",
        "        if y > ep:\n",
        "            break\n",
        "        m = m + 1\n",
        "    return m*gamma\n",
        "\n",
        "def compute_radius(MinDist,ClusterIndice):\n",
        "    cluster = np.unique(ClusterIndice)\n",
        "    nc = len(cluster)\n",
        "    cluster_rad = []\n",
        "    for i in range(nc):\n",
        "        currentcluster = np.where(ClusterIndice==cluster[i])[0]\n",
        "        cluster_rad.append(np.mean(MinDist[currentcluster]))\n",
        "    return cluster_rad\n",
        "\n",
        "def DCCA(sample,stdData,P_Summary,gamma,dim):\n",
        "    P_Center = P_Summary[:,0:dim]\n",
        "    P_F = P_Summary[:,dim]\n",
        "    gam1 = gamma\n",
        "    N1 = np.shape(sample)[0]\n",
        "    N2 = np.shape(P_Center)[0]\n",
        "    ep = 0.998\n",
        "    N = N1 + N2\n",
        "    temp = np.concatenate([sample,P_Center],axis=0)\n",
        "    Dist = Distance_Cal(temp)\n",
        "    while 1:\n",
        "        gam2 = gam1 + 5\n",
        "        den1 = []\n",
        "        den2 = []\n",
        "        for i in range(N):\n",
        "            Diff = np.power(Dist[i,0:N1],2)\n",
        "            temp1 = np.power(np.exp(-Diff/stdData),gam1)\n",
        "            temp2 = np.power(np.exp(-Diff/stdData),gam2)\n",
        "            sum1 = np.sum(temp1)\n",
        "            sum2 = np.sum(temp2)\n",
        "            if i<N1:\n",
        "                T1 = 0\n",
        "                T2 = 0\n",
        "                for j in range(N2):\n",
        "                    T1 += P_F[j]**(gam1/gamma)\n",
        "                    T2 += P_F[j]**(gam2/gamma)\n",
        "                s1 = sum1 + T1\n",
        "                s2 = sum2 + T2\n",
        "            else:\n",
        "                s1 = sum1 + P_F[i-N1]**(gam1/gamma)\n",
        "                s2 = sum2 + P_F[i-N1]**(gam2/gamma)\n",
        "            den1.append(s1)\n",
        "            den2.append(s2)\n",
        "        y = np.corrcoef(den1,den2)[0,1]\n",
        "        if y > ep:\n",
        "            break\n",
        "        gam1 = gam2\n",
        "    return gam1\n",
        "\n",
        "def TPC_Search(Dist,Pop_Index,Pop,radius,fitness):\n",
        "    # Extract the size of the population\n",
        "    [N,dim] = np.shape(Pop)\n",
        "    P = [] # Initialize the Peak Vector\n",
        "    P_fitness = []\n",
        "    i = 0\n",
        "    marked = []\n",
        "    co = []\n",
        "    OriginalIndice = Pop_Index\n",
        "    OriFit = fitness\n",
        "    TPC_Indice = OriginalIndice\n",
        "    PeakIndices = []\n",
        "    while 1:\n",
        "        #-------------Search for the local maximum-----------------#\n",
        "        SortIndice = np.argsort(fitness)\n",
        "        NewIndice = SortIndice[::-1]\n",
        "        Pop = Pop[NewIndice,:]\n",
        "        fitness = fitness[NewIndice]\n",
        "        OriginalIndice = OriginalIndice[NewIndice]\n",
        "        P.append(Pop[0,:])\n",
        "        P_fitness.append(fitness[0])\n",
        "        PeakIndices.append(np.where(OriFit==fitness[0])[0][0])\n",
        "        P_Indice = OriginalIndice[0]\n",
        "        Ind = AssigntoPeaks(Pop,Pop_Index,P,P_Indice,marked,radius,Dist)\n",
        "        marked.append(Ind)\n",
        "        marked.append(NewIndice[0])\n",
        "        if not Ind:\n",
        "            Ind = [NewIndice[0]]\n",
        "\n",
        "        TPC_Indice[Ind] = PeakIndices[i]\n",
        "        co.append(len(Ind))\n",
        "        TempFit = fitness\n",
        "        sum1 = 0\n",
        "        for j in range(len(Ind)):\n",
        "            sum1 += fitness[np.where(OriginalIndice==Ind[j])[0]]\n",
        "        for th in range(len(Ind)):\n",
        "            TempFit[np.where(OriginalIndice==Ind[th])[0]] = fitness[np.where(OriginalIndice==Ind[th])[0]]/(1+sum1)\n",
        "        fitness = TempFit\n",
        "        i = i + 1\n",
        "        if np.sum(co)>=N:\n",
        "            P = np.asarray(P)\n",
        "            P_fitness = np.asarray(P_fitness)\n",
        "            TPC_Indice = Close_Clusters(Pop,PeakIndices,Dist)\n",
        "            break\n",
        "    return P,P_fitness,TPC_Indice,PeakIndices\n",
        "\n",
        "def MergeInChunk(P,P_fitness,sample,gamma,stdData,Dist,TPC_Indice,PeakIndices):\n",
        "    \"\"\"Perform the Merge of TPCs witnin each data chunk\n",
        "    \"\"\"\n",
        "    # Num of TPCs\n",
        "    [Nc,dim] = np.shape(P)\n",
        "    NewP = []\n",
        "    NewP_fitness = []\n",
        "    marked= []\n",
        "    unmarked= []\n",
        "    Com = []\n",
        "\n",
        "    # Num of TPCs\n",
        "    Nc = np.shape(P)[0]\n",
        "    for i in range(Nc):\n",
        "        MinDist = np.inf\n",
        "        MinIndice = 100000\n",
        "        if i not in marked:\n",
        "            for j in range(Nc):\n",
        "                if j!=i and j not in marked:\n",
        "                    d = np.linalg.norm(P[j,:]-P[i,:])\n",
        "                    if d < MinDist:\n",
        "                        MinDist = d\n",
        "                        MinIndice = j\n",
        "            if MinIndice <= Nc:\n",
        "                MinIndice = int(MinIndice)\n",
        "                Merge = True\n",
        "                Neighbor = P[MinIndice][:]\n",
        "                X = (Neighbor + P[i,:])/2\n",
        "                X = np.reshape(X,(1,np.shape(P)[1]))\n",
        "                fitX = Fitness_Cal(sample,X,stdData,gamma)\n",
        "                fitP = P_fitness[i]\n",
        "                fitN = P_fitness[MinIndice]\n",
        "                if fitX < 1*min(fitN,fitP):\n",
        "                    Merge = False\n",
        "                if Merge:\n",
        "                    Com.append([i,MinIndice])\n",
        "                    marked.append(MinIndice)\n",
        "                    marked.append(i)\n",
        "                else:\n",
        "                    unmarked.append(i)\n",
        "    Com = np.asarray(Com)\n",
        "    # Number of Possible Merges:\n",
        "    Nm = np.shape(Com)[0]\n",
        "    for k in range(Nm):\n",
        "        if P_fitness[Com[k,0]] >= P_fitness[Com[k,1]]:\n",
        "            NewP.append(P[Com[k,0],:])\n",
        "            NewP_fitness.append(P_fitness[Com[k,0]])\n",
        "        else:\n",
        "            NewP.append(P[Com[k,1],:])\n",
        "            NewP_fitness.append(P_fitness[Com[k,1]])\n",
        "    # Add Unmerged TPCs to the NewP\n",
        "    for n in range(Nc):\n",
        "        if n not in Com:\n",
        "            NewP.append(P[n,:])\n",
        "            NewP_fitness.append(P_fitness[n])\n",
        "    NewP = np.asarray(NewP)\n",
        "    NewP_fitness = np.asarray(NewP_fitness)\n",
        "    return NewP,NewP_fitness\n",
        "\n",
        "def MergeOnline(P,P_fitness,P_Summary,PreStd,sample,gamma,stdData):\n",
        "    \"\"\"Perform the Merge of Clusters Between Historical and New Clusters\n",
        "    \"\"\"\n",
        "    # Num of TPCs\n",
        "    [Nc,dim] = np.shape(P)\n",
        "    NewP = []\n",
        "    NewP_fitness = []\n",
        "    marked= []\n",
        "    unmarked= []\n",
        "    Com = []\n",
        "    for i in range(Nc):\n",
        "        MinDist = np.inf\n",
        "        MinIndice = 100000\n",
        "        if i not in marked:\n",
        "            for j in range(Nc):\n",
        "                if j!=i and j not in marked:\n",
        "                    d = np.linalg.norm(P[j,:]-P[i,:])\n",
        "                    if d < MinDist:\n",
        "                        MinDist = d\n",
        "                        MinIndice = j\n",
        "            if MinIndice < Nc:\n",
        "                Merge = True\n",
        "                Neighbor = P[MinIndice][:]\n",
        "                X = (Neighbor + P[i][:])/2\n",
        "                X = np.reshape(X,(1,np.shape(P)[1]))\n",
        "                RfitX = Fitness_Cal(sample,X,stdData,gamma)\n",
        "                fitX = fitness_update(P_Summary,X,RfitX,PreStd,gamma,stdData)\n",
        "                fitP = P_fitness[i]\n",
        "                fitN = P_fitness[MinIndice]\n",
        "                if fitX < 1*min(fitN,fitP):\n",
        "                    Merge = False\n",
        "                if Merge:\n",
        "                    Com.append([i,MinIndice])\n",
        "                    marked.append(MinIndice)\n",
        "                    marked.append(i)\n",
        "                else:\n",
        "                    unmarked.append(i)\n",
        "    Com = np.asarray(Com)\n",
        "    # Number of Possible Merges:\n",
        "    Nm = np.shape(Com)[0]\n",
        "    for k in range(Nm):\n",
        "        if P_fitness[Com[k,0]] >= P_fitness[Com[k,1]]:\n",
        "            NewP.append(P[Com[k,0]][:])\n",
        "            NewP_fitness.append(P_fitness[Com[k,0]])\n",
        "        else:\n",
        "            NewP.append(P[Com[k,1]][:])\n",
        "            NewP_fitness.append(P_fitness[Com[k,1]])\n",
        "    # Add Unmerged TPCs to the NewP\n",
        "    for n in range(Nc):\n",
        "        if n not in Com:\n",
        "            NewP.append(P[n][:])\n",
        "            NewP_fitness.append(P_fitness[n])\n",
        "    NewP = np.asarray(NewP)\n",
        "    NewP_fitness = np.asarray(NewP_fitness)\n",
        "    return NewP,NewP_fitness\n",
        "\n",
        "def CE_InChunk(sample,P,P_fitness,stdData,gamma,Dist,TPC_Indice,PeakIndices):\n",
        "    while 1:\n",
        "        HistP = P\n",
        "#        HistPF = P_fitness\n",
        "        P,P_fitness = MergeInChunk(P,P_fitness,sample,gamma,stdData,Dist,TPC_Indice,PeakIndices)\n",
        "        if np.shape(P)[0] == np.shape(HistP)[0]:\n",
        "            break\n",
        "    return P,P_fitness\n",
        "\n",
        "def CE_Online(sample,P_Summary,P,P_fitness,stdData,gamma,PreStd):\n",
        "    dim = np.shape(P)[1]\n",
        "    # Concatenate the historical and new clusters together\n",
        "    PC = np.concatenate([P_Summary[:,0:dim],P])\n",
        "    RPF = Fitness_Cal(sample,PC,stdData,gamma)\n",
        "    PF = fitness_update(P_Summary,PC,RPF,PreStd,gamma,stdData)\n",
        "\n",
        "    while 1:\n",
        "        HistPC = PC\n",
        "        PC,PF = MergeOnline(PC,PF,P_Summary,PreStd,sample,gamma,stdData)\n",
        "        RPF = Fitness_Cal(sample,PC,stdData,gamma)\n",
        "        PF = fitness_update(P_Summary,PC,RPF,PreStd,gamma,stdData)\n",
        "        if np.shape(PC)[0] == np.shape(HistPC)[0]:\n",
        "            break\n",
        "    return PC,PF\n",
        "\n",
        "def ClusterValidation(sample,P):\n",
        "    while 1:\n",
        "        NewP = []\n",
        "        PreP = P\n",
        "        [R_d,RIndice] = Cluster_Assign(sample,P)\n",
        "\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            Temp = np.where(RIndice==i)\n",
        "            Temp = np.asarray(Temp)\n",
        "            if np.shape(Temp)[1]>2:\n",
        "                NewP.append(P[i][:])\n",
        "        P = NewP\n",
        "        if np.shape(P)[0] == np.shape(PreP)[0]:\n",
        "            break\n",
        "    return np.asarray(P)\n",
        "\n",
        "def ClusterSummary(P,PF,P_Summary,sample):\n",
        "    dim = np.shape(sample)[1]\n",
        "    Rp = AverageDist(P,P_Summary,sample,dim)\n",
        "    P = np.asarray(P)\n",
        "    PF = [PF]\n",
        "    PF = np.asarray(PF)\n",
        "    Rp = np.reshape(Rp,(np.shape(P)[0],1))\n",
        "    PCluster = np.concatenate([P,PF.T],axis=1)\n",
        "    PCluster = np.concatenate([PCluster,Rp],axis=1)\n",
        "    P_Summary = PCluster\n",
        "    return P_Summary\n",
        "\n",
        "def StoreInf(PF,PFS,PreStd,stdData):\n",
        "    PreStd.append(stdData)\n",
        "    PFS.append(PF)\n",
        "    return PreStd,PFS\n",
        "\n",
        "\n",
        "#--------------------Cluster Radius Computation and Update--------------------#\n",
        "def AverageDist(P, P_Summary, sample, dim):\n",
        "    P = P\n",
        "    # Obtain the assignment of clusters\n",
        "    [distance,indices] = Cluster_Assign(sample,P)\n",
        "    rad1 = []\n",
        "    # if the summary of clusters is not empty\n",
        "    if len(P_Summary)>0:\n",
        "\n",
        "        PreP = P_Summary[:,0:dim] # Hstorical Cluster Center vector\n",
        "        PreR = P_Summary[:,dim+1]\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            if np.shape(np.where(indices==i))[1] >1:\n",
        "                SumD1 = 0\n",
        "                Count1 = 0\n",
        "                for j in range(np.shape(sample)[0]):\n",
        "                    if indices[j] == i:\n",
        "                        SumD1 += distance[j]\n",
        "                        Count1 += 1\n",
        "                rad1.append(SumD1 / Count1)\n",
        "            else:\n",
        "                C_d = []\n",
        "                for k in range(np.shape(PreP)[0]):\n",
        "                    C_d.append(np.linalg.norm(P[i][:] - PreP[k][:]))\n",
        "                CI = np.argmin(C_d)\n",
        "                rad1.append(PreR[CI])\n",
        "    elif not P_Summary:\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            SumD1 = 0\n",
        "            Count1 = 0\n",
        "            for j in range(np.shape(sample)[0]):\n",
        "                if indices[j] == i:\n",
        "                    SumD1 += distance[j]\n",
        "                    Count1 += 1\n",
        "            rad1.append(SumD1/Count1)\n",
        "    return np.asarray(rad1)\n",
        "\n",
        "def AssigntoPeaks(pop,pop_index,P,P_I,marked,radius,Dist):\n",
        "    temp = []\n",
        "    [N,L] = np.shape(pop)\n",
        "    for i in range(N):\n",
        "        distance = Dist[i,P_I]\n",
        "        if not np.any(marked==pop_index[i]):\n",
        "            if distance <= radius:\n",
        "                temp.append(pop_index[i])\n",
        "    indices = temp\n",
        "    return indices\n",
        "\n",
        "def Close_Clusters(pop,PeakIndices,Dist):\n",
        "    P = pop[PeakIndices][:]\n",
        "    C_Indices = np.arange(0,np.shape(pop)[0])\n",
        "    for i in range(np.shape(pop)[0]):\n",
        "        temp_dist = Dist[i][PeakIndices]\n",
        "        C_Indices[i] = PeakIndices[np.argmin(temp_dist)]\n",
        "    return C_Indices\n",
        "\n",
        "\n",
        "def Cluster_Assign(sample,P):\n",
        "    # Number of samples\n",
        "    N = np.shape(sample)[0]\n",
        "    # Number of Clusters at t\n",
        "    Np = np.shape(P)[0]\n",
        "    MinDist = []\n",
        "    MinIndice = []\n",
        "    for i in range(N):\n",
        "        d = []\n",
        "        for j in range(Np):\n",
        "            d.append(np.linalg.norm(sample[i][:]-P[j][:]))\n",
        "        if len(d)<=1:\n",
        "            tempD = d\n",
        "            tempI = 0\n",
        "        else:\n",
        "            tempD = np.min(d)\n",
        "            tempI = np.argmin(d)\n",
        "\n",
        "        MinDist.append(tempD)\n",
        "        MinIndice.append(tempI)\n",
        "    MinDist = np.asarray(MinDist)\n",
        "    MinIndice = np.asarray(MinIndice)\n",
        "    return MinDist,MinIndice\n",
        "\n",
        "def DiversityFetch1(candidate_fet1, current, priority1, interd1, dth, fetchsize):\n",
        "    fetch1 = []\n",
        "    num_center = fetchsize\n",
        "    chunked_dist1 = interd1[candidate_fet1]\n",
        "    chunked_dist = chunked_dist1[:,candidate_fet1]\n",
        "    for i in range(num_center):\n",
        "        top_idx = np.argmax(priority1)\n",
        "        fetch1.append(current[candidate_fet1[top_idx]])\n",
        "        neighbordist = chunked_dist[top_idx,:]\n",
        "        neighboridx = np.where(neighbordist <= dth)[0]\n",
        "        priority1[top_idx] = priority1[top_idx] / (1 + 20*np.sum(priority1[neighboridx]))\n",
        "        priority1[neighboridx] = priority1[neighboridx] / (1 + np.sum(priority1[neighboridx]))\n",
        "    fetch1 = np.asarray(fetch1)\n",
        "    fetch1 = fetch1.astype(int)\n",
        "    return fetch1\n",
        "\n",
        "def DiversityFetch2(candidate_fet2, current, priority2, interd1, dth, fetchsize):\n",
        "    fetch2 = []\n",
        "    num_border = fetchsize\n",
        "    chunked_dist1 = interd1[candidate_fet2]\n",
        "    chunked_dist = chunked_dist1[:,candidate_fet2]\n",
        "    for i in range(num_border):\n",
        "        top_idx = np.argmax(priority2)\n",
        "        fetch2.append(current[candidate_fet2[top_idx]])\n",
        "        neighbordist = chunked_dist[top_idx][:]\n",
        "        neighboridx = np.where(neighbordist <= dth)[0]\n",
        "        priority2[top_idx] = priority2[top_idx] / (1 + 200*np.sum(priority2[neighboridx]))\n",
        "        priority2[neighboridx] = priority2[neighboridx] / (1 + np.sum(priority2[neighboridx]))\n",
        "    fetch2 = np.asarray(fetch2)\n",
        "    fetch2 = fetch2.astype(int)\n",
        "    return fetch2\n",
        "\n",
        "\n",
        "def fps_clustering():\n",
        "       [data, label] = Input()\n",
        "       [BufferSize, P_Summary, T, PFS, PreStd] = ParamSpe(data)\n",
        "       T = int(T)\n",
        "       gammaHist = []\n",
        "       PFS = []\n",
        "       PreMu = []\n",
        "       for t in range(T):\n",
        "           if t < T - 1:\n",
        "               sample = data[t * BufferSize:(t + 1) * BufferSize, :]\n",
        "           else:\n",
        "               sample = data[t * BufferSize:np.shape(data)[0]]\n",
        "           if t == 0:\n",
        "               AccSample = sample\n",
        "           else:\n",
        "               AccSample = np.concatenate([AccSample, sample])\n",
        "           dim = np.shape(sample)[1]\n",
        "           [stdData, pop_index, pop, radius, PreMu, PreStd] = PopInitial(sample, PreMu, PreStd, BufferSize)\n",
        "           # Initialize the fitness vector\n",
        "           fitness = np.zeros((len(pop_index), 1))\n",
        "           # Initialize the indices vector\n",
        "           indices = np.zeros((len(pop_index), 1))\n",
        "           Dist = Distance_Cal(sample)\n",
        "           if PreStd:\n",
        "               if PreStd[len(PreStd) - 1] > stdData:\n",
        "                   P = P_Summary[:, 0:dim]\n",
        "                   localFit = Fitness_Cal(sample, P, stdData, gamma)\n",
        "                   PF = fitness_update(P_Summary, P, localFit, PreStd, gamma, stdData)\n",
        "                   P_Summary = ClusterSummary(P, PF, P_Summary, sample)\n",
        "                   PFS.append(PF)\n",
        "                   PreStd.append(stdData)\n",
        "                   clustercenter = P\n",
        "                   [Assign, clusterindex] = Cluster_Assign(AccSample, P)\n",
        "                   continue\n",
        "           else:\n",
        "               gamma = CCA(sample, stdData, Dist)\n",
        "           gammaHist.append(gamma)\n",
        "           fitness = Fitness_Cal(sample, pop, stdData, gamma)\n",
        "           fitness = np.array(fitness)\n",
        "           P, P_fitness, TPC_Indice, PeakIndices = TPC_Search(Dist, pop_index, pop, radius, fitness)\n",
        "           P, P_fitness = CE_InChunk(sample, P, P_fitness, stdData, gamma, Dist, TPC_Indice, PeakIndices)\n",
        "           P_fitness = Fitness_Cal(sample, P, stdData, gamma)\n",
        "           P_fitness = fitness_update(P_Summary, P, P_fitness, PreStd, gamma, stdData)\n",
        "\n",
        "           if t == 0:\n",
        "               P = P\n",
        "               PF = np.asarray(P_fitness)\n",
        "           else:\n",
        "               P, P_fitness = CE_Online(sample, P_Summary, P, P_fitness, stdData, gamma, PreStd)\n",
        "               PF = np.asarray(P_fitness)\n",
        "           P_Summary = ClusterSummary(P, PF, P_Summary, sample)\n",
        "           PreStd, PFS = StoreInf(PF, PFS, PreStd, stdData)\n",
        "       [MinDist, ClusterIndice] = Cluster_Assign(AccSample, P)\n",
        "       return AccSample, P, ClusterIndice, data, label\n",
        "\n",
        "def active_query(samples, cluster_centers, cluster_idx, label_budget):\n",
        "       query_idx = []\n",
        "       dist_cluster = squareform(pdist(cluster_centers))\n",
        "\n",
        "       for i in range(np.shape(cluster_centers)[0]):\n",
        "           curr_cluster = np.where(cluster_idx == i)[0]\n",
        "           curr_dist = squareform(pdist(samples[curr_cluster]))\n",
        "           num_queries = round(label_budget * len(curr_dist) / np.shape(samples)[0])\n",
        "           num_nei = round(len(curr_cluster) ** 0.5)\n",
        "           knei_dist, query_priority = [], []\n",
        "           temp_interdist = dist_cluster[i, :]\n",
        "           if len(curr_cluster) < 2:\n",
        "               continue\n",
        "           temp_neigh1 = cluster_centers[np.argsort(temp_interdist)[0], :]\n",
        "           temp_neigh2 = cluster_centers[np.argsort(temp_interdist)[1], :]\n",
        "           for j in range(len(curr_cluster)):\n",
        "               query_priority.append(1 + exp(-np.linalg.norm(samples[curr_cluster[j], :] - cluster_centers[i, :])))\n",
        "               knei_dist.append(np.mean(curr_dist[j, :num_nei]))\n",
        "           sortIndex1 = np.argsort(query_priority)\n",
        "           sortIndex1 = sortIndex1[::-1]\n",
        "           dth = np.mean(knei_dist)\n",
        "           query_priority = np.array(query_priority)\n",
        "           fet1 = DiversityFetch1(sortIndex1[:round(len(query_priority) / 2)],\n",
        "                                  curr_cluster,\n",
        "                                  query_priority[sortIndex1[:round(len(query_priority) / 2)]],\n",
        "                                  curr_dist, dth, round(num_queries * 0.5))\n",
        "           fil_index = sortIndex1[-int(round(len(query_priority) / 2)):]\n",
        "           d2 = []\n",
        "           for k in range(len(fil_index)):\n",
        "               temp_d1 = np.linalg.norm(samples[curr_cluster[fil_index[k]], :] - temp_neigh1)\n",
        "               temp_d2 = np.linalg.norm(samples[curr_cluster[fil_index[k]], :] - temp_neigh2)\n",
        "               temp_ratio1 = max(temp_d1, temp_d2) / min(temp_d1, temp_d2)\n",
        "               d2.append(temp_ratio1)\n",
        "           sortIndex2 = np.argsort(d2)\n",
        "           candidate_fet2 = fil_index[sortIndex2[:int(round(num_queries * 0.8))]]\n",
        "           sum_dist = []\n",
        "           for ii in range(len(candidate_fet2)):\n",
        "               candidate_d1 = np.linalg.norm(samples[curr_cluster[candidate_fet2[ii]], :] - temp_neigh1)\n",
        "               candidate_d2 = np.linalg.norm(samples[curr_cluster[candidate_fet2[ii]], :] - temp_neigh1)\n",
        "               sum_dist.append(1 + 1 / (1 + candidate_d1 + candidate_d2))\n",
        "           sortIndex3 = np.argsort(sum_dist)\n",
        "           sortIndex3 = sortIndex3[::-1]\n",
        "           sum_dist = np.array(sum_dist)\n",
        "           fet2 = DiversityFetch2(candidate_fet2, curr_cluster,\n",
        "                                  sum_dist, curr_dist, dth,\n",
        "                                  round(num_queries * 0.5))\n",
        "           query_idx = np.append(query_idx, fet1)\n",
        "           query_idx = np.append(query_idx, fet2)\n",
        "       return query_idx\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Specify the label ratio here\n",
        "    label_ratiovalues = [0.10]\n",
        "    Result1 = {}\n",
        "    Result2 = {}\n",
        "    AccSample, P, ClusterIndice, data, label = fps_clustering()\n",
        "    for it in range(len(label_ratiovalues)):\n",
        "        label_ratio = label_ratiovalues[it]\n",
        "        sample_size = np.shape(AccSample)[0]\n",
        "        label_budget = round(label_ratio * sample_size)\n",
        "        query_indices = active_query(AccSample, P, ClusterIndice, label_budget)\n",
        "        FetchIndex = query_indices.astype(int)\n",
        "        sample_index = np.arange(0, np.shape(AccSample)[0])\n",
        "        sample_index = sample_index.astype(int)\n",
        "        FetchIndex = FetchIndex.astype(int)\n",
        "        UnlabeledIndex = []\n",
        "        for s_idx in sample_index:\n",
        "            if s_idx not in FetchIndex:\n",
        "                UnlabeledIndex = np.append(UnlabeledIndex, s_idx)\n",
        "        UnlabeledIndex = UnlabeledIndex.astype(int)\n",
        "        sample_Fetch = AccSample[FetchIndex][:]\n",
        "        sample_Unlabeled = AccSample[UnlabeledIndex][:]\n",
        "        label_Fetch = label[FetchIndex]\n",
        "        label_Unlabeled = label[UnlabeledIndex]\n",
        "        new_fetchX = sample_Fetch\n",
        "        new_fetchY = label_Fetch\n",
        "\n",
        "        clf1 = KNeighborsClassifier(n_neighbors=3)\n",
        "        clf3 = LinearSVC()\n",
        "        clf1 = clf1.fit(new_fetchX, new_fetchY)\n",
        "        clf3 = clf3.fit(new_fetchX, new_fetchY)\n",
        "        pred_label1 = clf1.predict(sample_Unlabeled)\n",
        "        pred_label3 = clf3.predict(sample_Unlabeled)\n",
        "\n",
        "        Acc1 = clf1.score(sample_Unlabeled, label_Unlabeled)\n",
        "        Acc3 = clf3.score(sample_Unlabeled, label_Unlabeled)\n",
        "        F1_1 = f1_score(label_Unlabeled, pred_label1, average='macro')\n",
        "        F1_3 = f1_score(label_Unlabeled, pred_label3, average='macro')\n",
        "        P1_1 = precision_score(label_Unlabeled, pred_label1, average='macro')\n",
        "        P1_3 = precision_score(label_Unlabeled, pred_label3, average='macro')\n",
        "        R1_1 = recall_score(label_Unlabeled, pred_label1, average='macro')\n",
        "        R1_3 = recall_score(label_Unlabeled, pred_label3, average='macro')\n",
        "        temp_result1 = [Acc1, F1_1, P1_1, R1_1]\n",
        "        temp_result2 = [Acc3, F1_3, P1_3, R1_3]\n",
        "        Result1[str(label_ratiovalues[it])] = temp_result1\n",
        "        Result2[str(label_ratiovalues[it])] = temp_result2\n",
        "\n",
        "    print(\"-----------------Results-------------------\")\n",
        "    print(\"Results for KNN:\", Result1)\n",
        "    print(\"Results for SVM:\", Result2)\n",
        "    end = time.time()\n",
        "    ExecutionTime = end - start\n",
        "    print('The total Extection Time: ' + str(ExecutionTime))"
      ]
    }
  ]
}